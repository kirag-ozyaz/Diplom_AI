import sys
import json
import re
import uuid
from pathlib import Path
from datetime import datetime

def parse_header(lines):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫ —Ñ–∞–π–ª–∞ —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –ü–£–≠.
    –û–∂–∏–¥–∞–µ—Ç—Å—è:
    1. –ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏
    2. –†–∞–∑–¥–µ–ª (–Ω–æ–º–µ—Ä)
    3. –†–∞–∑–¥–µ–ª (–Ω–∞–∑–≤–∞–Ω–∏–µ)
    4. –ì–ª–∞–≤–∞ (–Ω–æ–º–µ—Ä)
    5. –ì–ª–∞–≤–∞ (–Ω–∞–∑–≤–∞–Ω–∏–µ)
    """
    metadata = {
        "document": "–ü–£–≠",
        "section_number": "",
        "section_title": "",
        "chapter_number": "",
        "chapter_title": "",
        "source_file": ""
    }

    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –Ω–∞—á–∞–ª–µ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    clean_lines = [line.strip() for line in lines if line.strip()]

    if len(clean_lines) >= 5:
        metadata["document"] = clean_lines[0].replace("#", "").strip()
        # –°—Ç—Ä–æ–∫–∞ 2 –º–æ–∂–µ—Ç –±—ã—Ç—å "–†–∞–∑–¥–µ–ª 1" –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ "1", –ø—ã—Ç–∞–µ–º—Å—è –≤—ã—á–ª–µ–Ω–∏—Ç—å –Ω–æ–º–µ—Ä
        sec_line = clean_lines[1].replace("#", "").strip()
        metadata["section_number"] = re.search(r'\d+', sec_line).group(0) if re.search(r'\d+', sec_line) else sec_line
        metadata["section_title"] = clean_lines[2].replace("#", "").strip()

        chap_line = clean_lines[3].replace("#", "").strip()
        metadata["chapter_number"] = re.search(r'[\d.]+', chap_line).group(0) if re.search(r'[\d.]+',
                                                                                           chap_line) else chap_line
        metadata["chapter_title"] = clean_lines[4].replace("#", "").strip()

    return metadata


def is_clause_start(line):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å –Ω–æ–º–µ—Ä–∞ –ø—É–Ω–∫—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1.2.1.)"""
    # –ü–∞—Ç—Ç–µ—Ä–Ω: —Ü–∏—Ñ—Ä—ã.—Ü–∏—Ñ—Ä—ã.—Ü–∏—Ñ—Ä—ã. (–≤–æ–∑–º–æ–∂–Ω–æ —Å –ø—Ä–æ–±–µ–ª–æ–º –ø–æ—Å–ª–µ)
    pattern = r'^\s*\d+\.\d+\.\d+\.\s*'
    return bool(re.match(pattern, line))


def extract_clause_id(line):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID –ø—É–Ω–∫—Ç–∞ –∏–∑ —Å—Ç—Ä–æ–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '1.2.1')"""
    match = re.match(r'^\s*(\d+\.\d+\.\d+)\.', line)
    return match.group(1) if match else None


def chunk_document(content, metadata):
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏.
    –õ–æ–≥–∏–∫–∞:
    - –ö–∞–∂–¥—ã–π –Ω–æ–≤—ã–π –ø—É–Ω–∫—Ç (X.Y.Z.) –Ω–∞—á–∏–Ω–∞–µ—Ç –Ω–æ–≤—ã–π —á–∞–Ω–∫.
    - –¢–∞–±–ª–∏—Ü—ã –∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤–∫–ª—é—á–∞—é—Ç—Å—è –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫.
    - –ó–∞–≥–æ–ª–æ–≤–∫–∏ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–æ–≤ (# ...) –ø—Ä–∏–∫—Ä–µ–ø–ª—è—é—Ç—Å—è –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —á–∞–Ω–∫—É –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    """
    lines = content.split('\n')
    chunks = []

    current_chunk_text = []
    current_clause_id = None
    current_subsection_title = None

    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–ø–µ—Ä–≤—ã–µ 5 –Ω–µ–ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤ metadata,
    # –Ω–æ –≤ —Ñ–∞–π–ª–µ –æ–Ω–∏ –µ—Å—Ç—å, –Ω—É–∂–Ω–æ –∏—Ö –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ç–µ–ª–∞)
    # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –±—É–¥–µ–º —Å–æ–±–∏—Ä–∞—Ç—å —Ç–µ–ª–æ, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –ø–µ—Ä–≤—ã–µ 5 –∑–Ω–∞—á–∏–º—ã—Ö —Å—Ç—Ä–æ–∫ –≥–ª–æ–±–∞–ª—å–Ω–æ,
    # –µ—Å–ª–∏ –æ–Ω–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏, –ª–∏–±–æ –ø—Ä–æ—Å—Ç–æ –Ω–∞—á–Ω–µ–º —Å–±–æ—Ä–∫—É —Å –ø–µ—Ä–≤–æ–≥–æ –ø—É–Ω–∫—Ç–∞.

    header_skipped = False
    header_count = 0

    for line in lines:
        stripped = line.strip()
        if not stripped:
            if current_chunk_text:
                current_chunk_text.append(line)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞
            continue

        # –ü—Ä–æ–ø—É—Å–∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Ñ–∞–π–ª–∞
        if not header_skipped:
            if header_count < 5:
                header_count += 1
                continue
            else:
                header_skipped = True

        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–∞ (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å #, –Ω–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—É–Ω–∫—Ç–æ–º)
        if stripped.startswith('#') and not is_clause_start(stripped):
            # –ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —á–∞–Ω–∫
            if current_chunk_text:
                chunks.append(
                    create_chunk_obj(current_clause_id, current_chunk_text, current_subsection_title, metadata))
                current_chunk_text = []

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–∞ –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —á–∞–Ω–∫–æ–≤
            current_subsection_title = stripped.replace('#', '').strip()
            continue

        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –Ω–æ–≤–æ–≥–æ –ø—É–Ω–∫—Ç–∞ (1.X.Y.)
        if is_clause_start(stripped):
            # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø—Ä–µ–¥—ã–¥—É—â–∏–π –ø—É–Ω–∫—Ç), —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ
            if current_chunk_text:
                chunks.append(
                    create_chunk_obj(current_clause_id, current_chunk_text, current_subsection_title, metadata))
                current_chunk_text = []

            current_clause_id = extract_clause_id(stripped)
            current_chunk_text.append(stripped)
            continue

        # –ï—Å–ª–∏ –º—ã –≤–Ω—É—Ç—Ä–∏ –ø—É–Ω–∫—Ç–∞ (–∏–ª–∏ –¥–æ –ø–µ—Ä–≤–æ–≥–æ –ø—É–Ω–∫—Ç–∞, –µ—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–∞—Ä—É—à–µ–Ω–∞)
        if current_clause_id or current_chunk_text:
            current_chunk_text.append(line)
        else:
            # –¢–µ–∫—Å—Ç –¥–æ –ø–µ—Ä–≤–æ–≥–æ –ø—É–Ω–∫—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–≤–µ–¥–µ–Ω–∏–µ –≥–ª–∞–≤—ã), atribu–µ–º –∫ "0.0.0" –∏–ª–∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            # –î–ª—è –ü–£–≠ –æ–±—ã—á–Ω–æ —Å—Ä–∞–∑—É –∏–¥—É—Ç –ø—É–Ω–∫—Ç—ã, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π —Å–æ–±–∏—Ä–∞–µ–º –≤ –±—É—Ñ–µ—Ä
            current_chunk_text.append(line)
            if not current_clause_id:
                current_clause_id = f"{metadata['chapter_number']}.intro"

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
    if current_chunk_text:
        chunks.append(create_chunk_obj(current_clause_id, current_chunk_text, current_subsection_title, metadata))

    return chunks


def create_chunk_obj(clause_id, text_lines, subsection_title, metadata):
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç JSON-–æ–±—ä–µ–∫—Ç –¥–ª—è —á–∞–Ω–∫–∞"""
    text_content = "\n".join(text_lines).strip()

    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
    chunk_id = f"pue_{clause_id}_{uuid.uuid4().hex[:8]}"

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ç–∞–±–ª–∏—Ü –∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫
    has_tables = "# –¢–∞–±–ª–∏—Ü–∞" in text_content or "|" in text_content
    has_images = "![" in text_content

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞
    context_header = f"–ü–£–≠ –†–∞–∑–¥–µ–ª {metadata['section_number']}: {metadata['section_title']}. –ì–ª–∞–≤–∞ {metadata['chapter_number']}: {metadata['chapter_title']}."
    if subsection_title:
        context_header += f" –ü–æ–¥—Ä–∞–∑–¥–µ–ª: {subsection_title}."

    full_content = f"{context_header}\n\n{text_content}"

    chunk_data = {
        "id": chunk_id,
        "metadata": {
            "document": metadata["document"],
            "section_number": metadata["section_number"],
            "section_title": metadata["section_title"],
            "chapter_number": metadata["chapter_number"],
            "chapter_title": metadata["chapter_title"],
            "subsection_title": subsection_title,
            "clause_id": clause_id,
            "contains_tables": has_tables,
            "contains_images": has_images,
            "source_file": metadata["source_file"]
        },
        "content": full_content,
        "created_at": datetime.now().isoformat()
    }

    return chunk_data


def generate_chunked_file(md_path, output_dir):
    md_path = Path(md_path).resolve()

    if md_path.suffix.lower() != '.md':
        print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª '{md_path}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è .md —Ñ–∞–π–ª–æ–º.")
        sys.exit(1)

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"üìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {md_path.name}")

    # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        sys.exit(1)

    # –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    lines = content.split('\n')
    metadata = parse_header(lines)
    metadata["source_file"] = md_path.name

    # –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ
    chunks = chunk_document(content, metadata)

    # –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: –∏–º—è_–∏—Å—Ö–æ–¥–Ω–æ–≥–æ.chunked.jsonl
    output_filename = f"{md_path.stem}.chunked.jsonl"
    output_path = output_dir / output_filename

    # –ó–∞–ø–∏—Å—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSONL (–∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - –≤–∞–ª–∏–¥–Ω—ã–π JSON)
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            for chunk in chunks:
                json_line = json.dumps(chunk, ensure_ascii=False)
                f.write(json_line + '\n')

        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤.")
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        return str(output_path)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e}")
        sys.exit(1)

# –ü—É—Ç–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã)
DEFAULT_INPUT_FILE = r"D:\my-diplom\Diplom_AI\data\extracted\1.8.md"
DEFAULT_OUTPUT_DIR = r"D:\my-diplom\Diplom_AI\data\chunked"

# output_chunked_file = r"X:\–£—á–µ–±–∞_–£–ò–ò\–ò—Ç–æ–≥–æ–≤—ã_–ü—Ä–æ–µ–∫—Ç\data\chunked"
# input_extracted_file = r"X:\–£—á–µ–±–∞_–£–ò–ò\–ò—Ç–æ–≥–æ–≤—ã_–ü—Ä–æ–µ–∫—Ç\data\extracted\1.9.md"

if __name__ == "__main__":
    input_dir_arg = None
    output_dir_arg = None
    try:
        # –†–∞–±–æ—á–∞—è –≤–µ—Ä—Å–∏—è: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
        if len(sys.argv) == 2 and sys.argv[1].strip():
            input_dir_arg = sys.argv[1]
        elif len(sys.argv) == 3 and sys.argv[1].strip() and sys.argv[2].strip():
            input_dir_arg = sys.argv[1]
            output_dir_arg = sys.argv[2]
        else:
            # –î–ª—è —Ç–µ—Å—Ç–æ–≤: –∏—Å–ø–æ–ª—å–∑—É–µ–º file_docx
            input_dir_arg = DEFAULT_INPUT_FILE
            output_dir_arg = DEFAULT_OUTPUT_DIR

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        docx_path = Path(input_dir_arg)
        if not docx_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_dir_arg}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º output_dir
        if output_dir_arg:
            output_dir = Path(output_dir_arg)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            if not output_dir.exists():
                raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {output_dir_arg}")
            if not output_dir.is_dir():
                raise NotADirectoryError(f"–£–∫–∞–∑–∞–Ω–Ω—ã–π –ø—É—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π: {output_dir_arg}")
        else:
            output_dir = docx_path.parent

        md_path = output_dir / f"{docx_path.stem}.md"

        chunked_content = generate_chunked_file(input_dir_arg, output_dir)

        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(chunked_content)

        print(f"üìÑ Markdown: {md_path}")
    except FileNotFoundError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python md_to_chunked.py <—Ñ–∞–π–ª.docx> [output_dir]")
        sys.exit(1)
    except NotADirectoryError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python md_to_chunked.py <—Ñ–∞–π–ª.docx> [output_dir]")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python md_to_chunked.py <—Ñ–∞–π–ª.docx> [output_dir]")
        sys.exit(1)